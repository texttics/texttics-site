<!doctype html>
<html>
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KKG6C4ZW');</script>
  <!-- End Google Tag Manager -->

<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Analytics & Consent Mode -->
<script>
  // Define dataLayer + gtag *first*
  window.dataLayer = window.dataLayer || [];
  function gtag(){ dataLayer.push(arguments); }

  // 1) Set Consent Mode *defaults* BEFORE loading gtag.js
  gtag('consent', 'default', {
    'ad_storage': 'denied',
    'analytics_storage': 'denied',
    'ad_user_data': 'denied',
    'ad_personalization': 'denied',
    'wait_for_update': 500
  });

  // (optional hardening)
  gtag('set', 'url_passthrough', true);
  gtag('set', 'ads_data_redaction', true);
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-HKGZL0K924"></script>

<script>
  // 3) Initialize & configure GA4
  gtag('js', new Date());
  gtag('config', 'G-HKGZL0K924');
</script>

<!-- PyScript -->
<link rel="stylesheet" href="https://pyscript.net/releases/2024.9.1/core.css" />
<script type="module" src="https://pyscript.net/releases/2024.9.1/core.js"></script>


  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
      font: 16px/1.4 system-ui, sans-serif;
      max-width: 1400px;
      margin: 2rem auto;
      padding: 0 1rem;
      padding-bottom: 150px;
      background-color: #f9fafb;
      color: #111827;
    }
    h1 { text-align: center; color: #1f2937; }
    h2 { 
      margin-top: 2.5rem; 
      color: #1f2937;
      border-bottom: 1px solid #d1d5db;
      padding-bottom: 0.5rem;
      display: flex;
      align-items: center;
    }
    textarea { 
      width: 100%; 
      height: 220px; 
      box-sizing: border-box; 
      border: 1px solid #d1d5db;
      border-radius: 10px;
      padding: 1rem;
      font-size: 1.1rem;
      line-height: 1.5;
    }
    textarea:focus {
      outline: 2px solid #3b82f6;
      border-color: #3b82f6;
    }
    
    /* --- Main Stats Grid --- */
    .stats {
      display: grid;
      gap: .8rem;
      margin-top: 1rem;
      grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));
    }

pre#skeleton-output {
      display: none; /* Hidden by default, shown by Python */
      font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace;
      background: #f3f4f6;
      border: 1px solid #d1d5db;
      border-radius: 10px;
      padding: 1rem;
      font-size: 1.1rem;
      line-height: 1.5;
      color: #1f2937;
      white-space: pre-wrap;     /* Wrap long skeleton strings */
      word-break: break-all;     /* Break long strings */
      margin-top: 1rem;
    }

.changed { 
      background: #fff3cd; 
      color: #664d03;
      padding: 0 .15rem; 
      border-radius: .2rem; 
      font-weight: bold;
    }
    .card {
      padding: 1rem;
      border: 1px solid #e5e7eb;
      border-radius: 10px;
      background: #ffffff;
      box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
      text-align: left;
    }
    .card strong {
      display: block;
      font-size: 0.9rem;
      color: #374151;
      margin-bottom: 0.25rem;
    }
    .card div {
      font-size: 1.75rem;
      font-weight: 600;
      color: #111827;
    }

    /* --- Tooltip Styles --- */
    .tooltip {
      position: relative;
      display: inline-block;
      cursor: help;
      font-size: 1rem;
      font-weight: 600;
      color: #3b82f6;
      margin-left: 8px;
    }
    .tooltip .tooltip-text {
      visibility: hidden;
      opacity: 0;
      transition: opacity 0.3s;
      width: 320px;
      background-color: #1f2937;
      color: #fff;
      text-align: left;
      font-weight: normal;
      font-size: 0.9rem;
      line-height: 1.4;
      border-radius: 6px;
      padding: 10px;
      position: absolute;
      z-index: 10;
      bottom: 125%;
      left: 50%;
      margin-left: -160px; /* Center it (half of 320px width) */
    }
    .tooltip .tooltip-text::after {
      content: "";
      position: absolute;
      top: 100%;
      left: 50%;
      margin-left: -5px;
      border-width: 5px;
      border-style: solid;
      border-color: #1f2937 transparent transparent transparent;
    }
    .tooltip:hover .tooltip-text {
      visibility: visible;
      opacity: 1;
    }

    /* --- Tab Navigation --- */
    .tabs-nav {
      display: flex;
      gap: 0.5rem;
      margin-top: 1rem;
      border-bottom: 1px solid #d1d5db;
    }
    .tab-btn {
      padding: 0.5rem 1rem;
      border: 1px solid transparent;
      border-bottom: 0;
      border-radius: 6px 6px 0 0;
      cursor: pointer;
      background: #e5e7eb;
      color: #4b5563;
      font-weight: 500;
      transition: all 0.2s;
    }
    .tab-btn:hover {
      background: #f3f4f6;
    }
    .tab-btn.active {
      background: #ffffff;
      border-color: #d1d5db;
      color: #111827;
      border-bottom: 1px solid #ffffff;
      margin-bottom: -1px;
    }
    .tab-content {
      display: none; /* Hidden by default */
      padding: 1.5rem 0.5rem;
      border: 1px solid #d1d5db;
      border-top: 0;
      border-radius: 0 0 10px 10px;
      background: #ffffff;
    }
    .tab-content.active {
      display: grid; /* Use grid when active */
    }

    /* --- Minor Stats (Tab 3) --- */
    .stats-minor {
      grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
      gap: 0.75rem;
    }
    .minor-card {
      display: grid;
      grid-template-columns: 40px 1fr 40px;
      align-items: center;
      padding: 0.5rem 0.75rem;
      background: #f9fafb;
      border: 1px solid #e5e7eb;
      border-radius: 6px;
    }
    .minor-key {
      font-weight: 700;
      font-family: monospace;
      color: #1d4ed8;
    }
    .minor-alias {
      font-size: 0.8rem;
      color: #6b7280;
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
    }
    .minor-val {
      font-size: 1.25rem;
      font-weight: 600;
      color: #111827;
      text-align: right;
    }
    
    /* --- Toggle Switch (Module 2) --- */
    .toggle-switch {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-top: 1rem;
    }
    .toggle-switch label {
      font-weight: 500;
      color: #374151;
    }
    .switch {
      position: relative;
      display: inline-block;
      width: 50px;
      height: 28px;
    }
    .switch input { 
      opacity: 0;
      width: 0;
      height: 0;
    }
    .slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background-color: #ccc;
      transition: .4s;
      border-radius: 28px;
    }
    .slider:before {
      position: absolute;
      content: "";
      height: 20px;
      width: 20px;
      left: 4px;
      bottom: 4px;
      background-color: white;
      transition: .4s;
      border-radius: 50%;
    }
    input:checked + .slider {
      background-color: #2563eb;
    }
    input:checked + .slider:before {
      transform: translateX(22px);
    }

   
    /* --- Script Stats (Module 3) --- */
    #script-stats {
      grid-template-columns: repeat(auto-fill, minmax(120px, 1fr));
    }
    
    /* Responsive */
    @media (max-width: 600px) {
      body { margin: 1rem auto; padding: 0 0.5rem; }
      h2 { font-size: 1.25rem; }
      .stats, .stats-minor {
        grid-template-columns: 1fr;
      }
      .tabs-nav {
        flex-direction: column;
        gap: 0;
      }
      .tab-btn {
        border-radius: 6px 6px 0 0;
        margin-bottom: 0;
      }
      .tab-btn.active {
        margin-bottom: -1px;
      }
      .tab-content {
        padding: 1rem 0.5rem;
      }
    }
    
  </style>
</head>
<body>
  <!-- GTM Noscript -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KKG6C4ZW"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End GTM Noscript -->

  <!-- Hidden div for Python to store state -->
  <div id="app-state" style="display: none;">{"active_tab": "summary"}</div>

  <h1>Text...tics</h1>
  <h2>Text Input</h2>  
  <textarea id="text" py-input="update_all" placeholder="Paste or type here..."></textarea>
  
  <!-- === MODULE 1: COMPREHENSIVE CHARACTER ANALYSIS === -->
  <h2>
    Comprehensive Character Analysis
    <span class="tooltip">(?)
      <span class="tooltip-text">
        Provides a detailed breakdown of all characters based on the official Unicode Standard. The analysis is presented in three layers of detail.
      </span>
    </span>
  </h2>

<div class="toggle-switch">
    <label for="analysis-mode-toggle">Filtered (Legacy)</label>
    <label class="switch">
      <input type="checkbox" id="analysis-mode-toggle" py-click="update_all" checked>
      <span class="slider"></span>
    </label>
    <label for="analysis-mode-toggle">Honest (Full Partition)</label>
  </div>

    <!-- --- New Toggle-Switch  - Grapheme Cluster (EGC) Toggle --- -->

    <div class="toggle-switch">
    <label for="unit-mode-toggle">Code Points (Raw)</label>
    <label class="switch">
      <input type="checkbox" id="unit-mode-toggle" py-click="update_all">
      <span class="slider"></span>
    </label>
    <label for="unit-mode-toggle">Graphemes (Perceived)</label>
    </div>
    
  <!-- Tab Navigation -->
  <div class="tabs-nav">
    <button id="tab-btn-summary" class="tab-btn active" py-click="select_tab">Summary</button>
    <button id="tab-btn-major" class="tab-btn" py-click="select_tab">Major Categories</button>
    <button id="tab-btn-minor" class="tab-btn" py-click="select_tab">Full Breakdown (30)</button>
  </div>
  
  <!-- Tab 1: Summary (Default) -->
  <div id="tab-summary" class="tab-content active stats">
    <!-- This content is generated by Python -->
  </div>
  
  <!-- Tab 2: Major Categories -->
  <div id="tab-major" class="tab-content stats">
    <!-- This content is generated by Python -->
  </div>

  <!-- Tab 3: Full Breakdown (30) -->
  <div id="tab-minor" class="tab-content stats stats-minor">
    <!-- This content is generated by Python -->
  </div>

  <div id="grapheme-forensics-module" style="display: none;"> <h2>
            Grapheme Forensic Analysis
            <span class="tooltip">(?)
                <span class="tooltip-text">
                    A forensic breakdown of the physical structure of the
                    "perceived characters" (graphemes) in the text.
                </span>
            </span>
        </h2>
        <div id="grapheme-forensics-stats" class="stats">
            </div>
    </div>

  <!-- === MODULE 2: TOKEN SHAPE ANALYSIS (CLASS RUNS) === -->
  <h2 id="module-2-header">
    Token Shape Analysis (Class Runs)
    <span class="tooltip">(?)
      <span class="tooltip-text">
        Performs a Run-Length Analysis to count uninterrupted sequences (or 'runs') of the same character type. (e.g., 'Hello-100%' has 4 sequences in 'Major' mode).
      </span>
    </span>
  </h2>
  
  <!-- Toggle Switch -->
  <div class="toggle-switch">
    <label for="granularity-toggle">Major Categories</label>
    <label class="switch">
      <input type="checkbox" id="granularity-toggle" py-click="update_all">
      <span class="slider"></span>
    </label>
    <label for="granularity-toggle">Minor Categories</label>
  </div>
  
  <!-- Sequence Stats Output -->
  <div id="sequence-stats" class="stats">
    <!-- This content is generated by Python -->
  </div>

  <!-- === MODULE 3: SCRIPT ANALYSIS === -->
  <h2 id="module-3-header">
    Script Analysis
    <span class="tooltip">(?)
      <span class="tooltip-text">
        Counts all characters belonging to a specific Unicode Script property (e.g., Latin, Cyrillic, Han, etc.).
      </span>
    </span>
  </h2>
  
  <!-- Script Stats Output -->
<div id="script-stats" class="stats">
        </div>

    <h2 id="module-4-header">
        Forensic Analysis
        <span class="tooltip">(?)
            <span class="tooltip-text">
                Detects "invisible" or "deceptive" characters used to manipulate text, or flags text for potential data corruption.
            </span>
        </span>
    </h2>

    <div id="forensic-stats" class="stats">
        </div>

<h2 id="module-5-header">
        Full UCD Profile (UAX #44)
        <span class="tooltip">(?)
            <span class="tooltip-text">
                A "treasure trove" of other deterministic properties from the Unicode Character Database (UAX #44), such as Block, Age, and binary properties.
            </span>
        </span>
    </h2>

    <div id="uax44-stats" class="stats">
        </div>

<h2 id="module-6-header">
        UCD Deep Scan (Python)
        <span class="tooltip">(?)
            <span class="tooltip-text">
                A full, Python-based scan of every code point to extract
                complex UCD properties like Numeric_Value and Age.
                This scan may be slower on very large texts.
            </span>
        </span>
    </h2>

    <div id="ucd-deep-scan-stats" class="stats">
        </div>

<h2 id="module-7-header">
      Confusable &amp; Spoofing Analysis
      <span class="tooltip">(?)
          <span class="tooltip-text">
              Detects "confusable" characters used in spoofing attacks by
              mapping them to their "skeleton" (e.g., Cyrillic 'а' -> Latin 'a').
          </span>
      </span>
  </h2>
  <div id="confusable-stats" class="stats">
  </div>
  <pre id="skeleton-output"></pre>

<h2 id="module-8-header">
      Standardized Variant Analysis
      <span class="tooltip">(?)
          <span class="tooltip-text">
              Detects standardized variant sequences, such as emoji
              variation selectors (e.g., U+FE0F) that can
              invisibly change a character's appearance.
          </span>
      </span>
  </h2>
  
  <div id="intentional-stats-output" class="stats">
      </div>

  <pre id="intentional-skeleton-output"></pre>
  
  <!-- === PYTHONS SCRIPT === -->
  <script type="py">

from pyscript import window, document
import json
from pyodide.ffi import create_proxy
import unicodedata
import asyncio
from pyodide.http import pyfetch
    
# --- 1. DEFINE CATEGORIES (Unicode Standard) ---

# Tier 3: The 30 Minor General Categories
MINOR_CATEGORIES = {
    # Letters
    "Lu": r"\p{Lu}", "Ll": r"\p{Ll}", "Lt": r"\p{Lt}", "Lm": r"\p{Lm}", "Lo": r"\p{Lo}",
    # Marks
    "Mn": r"\p{Mn}", "Mc": r"\p{Mc}", "Me": r"\p{Me}",
    # Numbers
    "Nd": r"\p{Nd}", "Nl": r"\p{Nl}", "No": r"\p{No}",
    # Punctuation
    "Pc": r"\p{Pc}", "Pd": r"\p{Pd}", "Ps": r"\p{Ps}", "Pe": r"\p{Pe}", 
    "Pi": r"\p{Pi}", "Pf": r"\p{Pf}", "Po": r"\p{Po}",
    # Symbols
    "Sm": r"\p{Sm}", "Sc": r"\p{Sc}", "Sk": r"\p{Sk}", "So": r"\p{So}",
    # Separators
    "Zs": r"\p{Zs}", "Zl": r"\p{Zl}", "Zp": r"\p{Zp}",
    # Other
    "Cc": r"\p{Cc}", "Cf": r"\p{Cf}", "Cs": r"\p{Cs}", "Co": r"\p{Co}", "Cn": r"\p{Cn}"
}

# Tier 1: Derived/Intuitive Properties
DERIVED_PROPERTIES = {
    "Emoji": r"\p{RGI_Emoji}",
    "Whitespace": r"\p{White_Space}" 
}

# New Module: Script Properties (Simplified)
SCRIPTS = {
    "Latin": r"\p{sc=Latin}",
    "Common": r"\p{sc=Common}", # Punct, symbols, etc.
    "Inherited": r"\p{sc=Inherited}" # Combining marks
}


# NEW: Forensic Properties (Module 3 Add-on)
FORENSIC_PROPERTIES = {
    "Deprecated": r"\p{Deprecated}",                # <-- NEW
    "Noncharacter": r"\p{Noncharacter_Code_Point}", # <-- NEW
    "Ignorables (Invisible)": r"\p{Default_Ignorable_Code_Point}",
    # 'Deceptive Spaces' requires the 'v' flag for set operations
    "Deceptive Spaces": r"[\p{White_Space}&&[^ \n\r\t]]"
}

# NEW: UAX #44 'Gold' Properties (Module 5)
# These properties are supported by the JS RegExp engine
UAX44_PROPERTIES = {
    # Binary Properties
    "Dash (binary)": r"\p{Dash}",
    "Alphabetic (binary)": r"\p{Alphabetic}",

    # Script Properties (these are supported!)
    "Script: Cyrillic": r"\p{Script=Cyrillic}",
    "Script: Greek": r"\p{Script=Greek}",
    "Script: Han": r"\p{Script=Han}",
    "Script: Arabic": r"\p{Script=Arabic}",
    "Script: Hebrew": r"\p{Script=Hebrew}"
}
    
# Tier 2: The 7 Major General Categories (for Token Shape Analysis)
MAJOR_CATEGORIES_TEST = {
    "L (Letter)": window.RegExp.new(r"^\p{L}$", "u"),
    "M (Mark)": window.RegExp.new(r"^\p{M}$", "u"),
    "N (Number)": window.RegExp.new(r"^\p{N}$", "u"),
    "P (Punctuation)": window.RegExp.new(r"^\p{P}$", "u"),
    "S (Symbol)": window.RegExp.new(r"^\p{S}$", "u"),
    "Z (Separator)": window.RegExp.new(r"^\p{Z}$", "u"),
    "C (Other)": window.RegExp.new(r"^\p{C}$", "u")
}

# Aliases for the "Full Breakdown" (Tier 3) tab
ALIASES = {
    "Lu": "Uppercase Letter", "Ll": "Lowercase Letter", "Lt": "Titlecase Letter", "Lm": "Modifier Letter", "Lo": "Other Letter",
    "Mn": "Nonspacing Mark", "Mc": "Spacing Mark", "Me": "Enclosing Mark",
    "Nd": "Decimal Number", "Nl": "Letter Number", "No": "Other Number",
    "Pc": "Connector Punct.", "Pd": "Dash Punct.", "Ps": "Open Punct.", "Pe": "Close Punct.", 
    "Pi": "Initial Punct.", "Pf": "Final Punct.", "Po": "Other Punct.",
    "Sm": "Math Symbol", "Sc": "Currency Symbol", "Sk": "Modifier Symbol", "So": "Other Symbol",
    "Zs": "Space Separator", "Zl": "Line Separator", "Zp": "Paragraph Separator",
    "Cc": "Control", "Cf": "Format", "Cs": "Surrogate", "Co": "Private Use", "Cn": "Unassigned"
}

# --- 2. CREATE JS REGEXP OBJECTS (for performance) ---

# Compilers for Module 1 (Character Analysis)
# RE_MINOR is now RE_MINOR_LEGACY (all 30, for Module 2 and Legacy Mode)
RE_MINOR_LEGACY = {key: window.RegExp.new(val, "gu") for key, val in MINOR_CATEGORIES.items()}
# NEW: A 29-category dict for "Honest" mode (all *except* 'Cn')
RE_MINOR_29_HONEST = {k: v for k, v in RE_MINOR_LEGACY.items() if k != "Cn"}

RE_DERIVED = {key: window.RegExp.new(val, "gv") for key, val in DERIVED_PROPERTIES.items()} 
RE_SCRIPTS = {key: window.RegExp.new(val, "gu") for key, val in SCRIPTS.items()}

# NEW: Compilers for Forensic Module
RE_FORENSIC = {
    # These can be 'gu' for simple global counting
    "Deprecated": window.RegExp.new(FORENSIC_PROPERTIES["Deprecated"], "gu"),             # <-- NEW
    "Noncharacter": window.RegExp.new(FORENSIC_PROPERTIES["Noncharacter"], "gu"),         # <-- NEW
    "Ignorables (Invisible)": window.RegExp.new(FORENSIC_PROPERTIES["Ignorables (Invisible)"], "gu"),
    # This one MUST use 'gv' (global, v-flag) for the set operation
    "Deceptive Spaces": window.RegExp.new(FORENSIC_PROPERTIES["Deceptive Spaces"], "gv")
}

# NEW: Compilers for UAX #44 Module
RE_UAX44 = {
    key: window.RegExp.new(val, "gu") 
    for key, val in UAX44_PROPERTIES.items()
}

# NEW: Counter for Grapheme Forensics (counts \p{M})
RE_MARK_COUNTER = window.RegExp.new(r"\p{M}", "gu")

# --- NEW: Helper for Module 7 (Phase 3 Refactor) ---
# Finds all continuous runs of one or more letters or numbers
    # Finds continuous runs of letters, numbers, and symbols (for visual diff)
    # OPTIONAL: include punctuation runs in the visual diff as well
RE_LN_RUNS = window.RegExp.new(r"\p{L}+|\p{N}+|\p{S}+|\p{P}+", "gu")
    
# Testers for Module 2 (Token Shape Analysis)
TEST_MINOR = {key: window.RegExp.new(f"^{val}$", "u") for key, val in MINOR_CATEGORIES.items()}

# NEW: Grapheme Segmenter (Module 1 alternate)
# We create one segmenter instance to reuse
GRAPHEME_SEGMENTER = window.Intl.Segmenter.new("en", {"granularity": "grapheme"})

# --- NEW: Identifier Status (Allowed -> Restricted complement) ---

ALLOWED_RANGES = []      # list[(start,end)]
ALLOWED_STARTS = []      # for bisect
ALLOWED_ENDS = []        # for bisect

# --- NEW: Globals for Blocks.txt ---
BLOCK_RANGES_STORE = [] # list[(start, end, "Block Name")]
BLOCK_STARTS = []       # for bisect
BLOCK_ENDS = []         # for bisect

# --- NEW: Globals for DerivedAge.txt ---
AGE_RANGES_STORE = []   # list[(start, end, "Age")]
AGE_STARTS = []         # for bisect
AGE_ENDS = []           # for bisect

# --- NEW: Globals for IdentifierType.txt (Phase 2) ---
ID_TYPE_RANGES_STORE = [] # list[(start, end, "Type")]
ID_TYPE_STARTS = []       # for bisect
ID_TYPE_ENDS = []         # for bisect

# --- NEW: Globals for confusables.txt (Phase 3) ---
CONFUSABLES_MAP = {} # dict[int, str] mapping code point -> skeleton string

# --- NEW: Globals for StandardizedVariants.txt (Module 8) ---
VARIANT_BASE_CHARS = set() # Holds chars that can be varied (e.g., U+263A)
VARIATION_SELECTORS = set() # Holds the selectors themselves (e.g., U+FE0F)
    
def _finalize_allowed(ranges):
    from bisect import bisect_right
    # normalize & prep arrays for O(log n) lookups
    ranges.sort()
    # merge overlapping/adjacent ranges for speed
    merged = []
    for s, e in ranges:
        if not merged or s > merged[-1][1] + 1:
            merged.append([s, e])
        else:
            merged[-1][1] = max(merged[-1][1], e)
    # commit
    global ALLOWED_RANGES, ALLOWED_STARTS, ALLOWED_ENDS
    ALLOWED_RANGES = [(s, e) for s, e in merged]
    ALLOWED_STARTS = [s for s, _ in ALLOWED_RANGES]
    ALLOWED_ENDS   = [e for _, e in ALLOWED_RANGES]
    print(f"Loaded {len(ALLOWED_RANGES)} allowed ranges.")

def parse_identifier_status(txt: str):
    """Parse IdentifierStatus.txt and ONLY store Allowed ranges."""
    ranges = []
    for raw in txt.splitlines():
        # strip inline comments and whitespace
        line = raw.split('#', 1)[0].strip()
        if not line:
            continue
        
        parts = line.split(';', 2) # Split into max 2 parts
        if len(parts) < 2:
            continue
            
        code = parts[0].strip()
        status = parts[1].strip() # <-- GET THE STATUS
        
        # --- THIS IS THE FIX ---
        # We only add it to the list if it's 'Allowed'
        if status == 'Allowed':
            if '..' in code:
                a, b = code.split('..', 1)
                ranges.append((int(a, 16), int(b, 16)))
            else:
                cp = int(code, 16)
                ranges.append((cp, cp))
                
    _finalize_allowed(ranges) # This function is still correct

def is_restricted(char: str) -> bool:
    """True iff char NOT covered by any Allowed range."""
    if not ALLOWED_RANGES:
        # Until data loads, be conservative (this may flash Restricted=True
        # for a moment, but it's safer than False)
        return True
    import bisect
    cp = ord(char)
    # Find the index of the last range that *starts* at or before cp
    i = bisect.bisect_right(ALLOWED_STARTS, cp) - 1
    # Check if we are within that range's end
    return not (i >= 0 and cp <= ALLOWED_ENDS[i])

def find_in_ranges(cp: int, starts_list, ends_list, store_list):
    """Generic range finder using bisect.
    
    Finds the value for a code point 'cp' in the pre-parsed lists.
    Returns the value (e.g., "Basic Latin" or "6.1") or None.
    """
    if not starts_list:
        return None
    import bisect
    # Find the index of the last range that *starts* at or before cp
    i = bisect.bisect_right(starts_list, cp) - 1
    
    # Check if we are within that range's end
    if i >= 0 and cp <= ends_list[i]:
        # store_list[i] is a tuple (start, end, value)
        return store_list[i][2] 
    return None

def _parse_and_store_ranges(txt: str, global_store, global_starts, global_ends, dataset_name: str):
    """
    A generic parser for Unicode data files (e.g., Blocks.txt, DerivedAge.txt).
    Expects lines like: 0000..007F; Basic Latin
    """
    from bisect import bisect_right
    ranges = []
    for raw in txt.splitlines():
        line = raw.split('#', 1)[0].strip()
        if not line:
            continue
        
        parts = line.split(';', 1)
        if len(parts) < 2:
            continue
        code_range, value = parts[0].strip(), parts[1].strip()
        
        if '..' in code_range:
            a, b = code_range.split('..', 1)
            ranges.append((int(a, 16), int(b, 16), value))
        else:
            cp = int(code_range, 16)
            ranges.append((cp, cp, value))
    
    # Sort for bisect
    ranges.sort()
    
    # Clear and populate the global lists
    global_store.clear()
    global_starts.clear()
    global_ends.clear()
    
    for s, e, v in ranges:
        global_store.append((s, e, v))
        global_starts.append(s)
        global_ends.append(e)
    
    print(f"Loaded {len(ranges)} ranges for {dataset_name}.")

def parse_blocks(txt: str):
    """Parses Blocks.txt into its global stores."""
    global BLOCK_RANGES_STORE, BLOCK_STARTS, BLOCK_ENDS
    _parse_and_store_ranges(txt, BLOCK_RANGES_STORE, BLOCK_STARTS, BLOCK_ENDS, "Blocks")

def parse_derived_age(txt: str):
    """Parses DerivedAge.txt into its global stores."""
    global AGE_RANGES_STORE, AGE_STARTS, AGE_ENDS
    _parse_and_store_ranges(txt, AGE_RANGES_STORE, AGE_STARTS, AGE_ENDS, "DerivedAge")

# --- NEW: Parser for IdentifierType.txt (Phase 2) ---
def parse_identifier_type(txt: str):
    """Parses IdentifierType.txt into its global stores."""
    global ID_TYPE_RANGES_STORE, ID_TYPE_STARTS, ID_TYPE_ENDS
    _parse_and_store_ranges(txt, ID_TYPE_RANGES_STORE, ID_TYPE_STARTS, ID_TYPE_ENDS, "IdentifierType")

# --- NEW: Parser for confusables.txt (Phase 3) ---
def parse_confusables(txt: str):
    """
    Parses confusables.txt into a simple {code_point: "skeleton"} map.
    Format: 0430 ; 0061 ; MA # (Cyrillic 'a' -> Latin 'a')
    Format: 00C6 ; 0041 0045 ; MA # (Latin 'Æ' -> "AE")
    """
    global CONFUSABLES_MAP
    CONFUSABLES_MAP.clear()
    count = 0
    
    for raw in txt.splitlines():
        line = raw.split('#', 1)[0].strip()
        if not line:
            continue
            
        parts = line.split(';', 2) # Split max 2 times
        if len(parts) < 2:
            continue
        
        try:
            source_hex = parts[0].strip()
            skeleton_hex_list = parts[1].strip().split() # e.g., "0041 0045"
            
            source_cp = int(source_hex, 16)
            
            # Build the skeleton string
            skeleton_str = "".join(
                [chr(int(hex_val, 16)) for hex_val in skeleton_hex_list]
            )

            # --- THIS IS THE FIX ---
            # We must check the code points, not the characters, to
            # correctly handle case-only mappings (like I -> l)
            skeleton_cp_list = [int(hex_val, 16) for hex_val in skeleton_hex_list]
            is_identity = (len(skeleton_cp_list) == 1 and skeleton_cp_list[0] == source_cp)
            
            if not is_identity:
                CONFUSABLES_MAP[source_cp] = skeleton_str
                count += 1
                
        except Exception:
            pass # Ignore malformed lines
            
    print(f"Loaded {count} confusable mappings.")

# --- NEW: Parser for StandardizedVariants.txt (Module 8) ---
def parse_standardized_variants(txt: str):
    """
    Parses StandardizedVariants.txt into two sets:
    1. VARIANT_BASE_CHARS: Base characters that can be modified.
    2. VARIATION_SELECTORS: The modifier characters themselves.
    """
    global VARIANT_BASE_CHARS, VARIATION_SELECTORS
    VARIANT_BASE_CHARS.clear()
    VARIATION_SELECTORS.clear()
    
    base_mappings_count = 0
    
    for raw in txt.splitlines():
        line = raw.split('#', 1)[0].strip()
        if not line:
            continue
            
        parts = line.split(';', 1) # Split into "data" and "description"
        if len(parts) < 2:
            continue
        
        data_part = parts[0].strip()
        
        # The data part is "BaseHex SelectorHex"
        hex_codes = data_part.split() 
        
        if len(hex_codes) == 2:
            try:
                base_cp = int(hex_codes[0], 16)
                selector_cp = int(hex_codes[1], 16)
                
                VARIANT_BASE_CHARS.add(base_cp)
                VARIATION_SELECTORS.add(selector_cp)
                
                base_mappings_count += 1
            except ValueError:
                pass # Ignore malformed hex codes
                
    selector_count = len(VARIATION_SELECTORS)
    print(f"Loaded {base_mappings_count} variant base char mappings and {selector_count} unique variation selectors.")


    
async def load_unicode_data():
    """Fetches, parses, and then triggers a UI update."""
    
    async def fetch_file(filename):
        """Helper to fetch a file and return its text or None."""
        try:
            response = await pyfetch(f"./{filename}")
            if response.ok:
                return await response.string()
            else:
                print(f"Failed to load {filename}: {response.status}")
                return None
        except Exception as e:
            print(f"Error loading {filename}: {e}")
            return None

    try:
        # --- NEW: Fetch all files in parallel ---
        filenames = ["IdentifierStatus.txt", "Blocks.txt", "DerivedAge.txt", "IdentifierType.txt", "confusables.txt", "StandardizedVariants.txt"]
        print("Fetching Unicode data files...")
        results = await asyncio.gather(*[fetch_file(f) for f in filenames])
        
        id_status_txt, blocks_txt, derived_age_txt, id_type_txt, confusables_txt, standardized_variants_txt = results
        
        # --- Parse each file if it loaded ---
        if id_status_txt:
            if "Identifier_Status" in id_status_txt:
                parse_identifier_status(id_status_txt)
            else:
                print("IdentifierStatus.txt content invalid.")

        if blocks_txt:
            if "Block" in blocks_txt:
                parse_blocks(blocks_txt)
            else:
                print("Blocks.txt content invalid.")
        
        if derived_age_txt:
            if "Age" in derived_age_txt:
                parse_derived_age(derived_age_txt)
            else:
                print("DerivedAge.txt content invalid.")

    # --- NEW: Parse IdentifierType.txt ---
        if id_type_txt:
            if "Identifier_Type" in id_type_txt: # Sanity check
                parse_identifier_type(id_type_txt)
            else:
                print("IdentifierType.txt content invalid.")

    # --- NEW: Parse confusables.txt ---
        if confusables_txt:
            if "confusables" in confusables_txt: # Sanity check
                parse_confusables(confusables_txt)
            else:
                print("confusables.txt content invalid.")

    # --- NEW: Parse StandardizedVariants.txt ---
        if standardized_variants_txt:
            if "# StandardizedVariants" in standardized_variants_txt: # Check for the file header
                parse_standardized_variants(standardized_variants_txt)
            else:
                print("StandardizedVariants.txt content invalid.")
    
        # --- Final step: Trigger UI update ---
        # This call is now crucial as it runs after all data is parsed
        print("Unicode data loaded. Triggering UI update.")
        update_all() 

    except Exception as e:
        print(f"An error occurred during parallel data loading: {e}")

# --- End of replacement block --
    
# --- 3. HELPER FUNCTIONS ---

def count_matches(regex, text):
    """Counts matches of a JS RegExp object in a string."""
    # We must explicitly call the JS String.prototype.match method
    # and pass the Python string 'text' as the 'this' context.
    matches = window.String.prototype.match.call(text, regex)
    return len(matches) if matches else 0

def get_char_type(char, is_minor):
    """Classifies a single char as a Major or Minor category."""
    if is_minor:
        for key, regex in TEST_MINOR.items():
            if regex.test(char):
                return key
    else:
        for key, regex in MAJOR_CATEGORIES_TEST.items():
            if regex.test(char):
                return key
    return "NONE"

def get_state():
    """Gets the UI state from the hidden div."""
    try:
        return json.loads(document.getElementById("app-state").innerHTML)
    except:
        return {"active_tab": "summary"} # Default state

def set_state(key, value):
    """Sets a value in the UI state."""
    state = get_state()
    state[key] = value
    document.getElementById("app-state").innerHTML = json.dumps(state)

# --- 4. CORE LOGIC (MODULES) ---

def compute_comprehensive_stats(t: str, is_honest_mode: bool):
        """Module 1: Runs the 3-Tier analysis."""
# --- PASS 1: Count DERIVED Properties (Tier 1) ---
    # This is now common to both modes
        code_points_array = window.Array.from_(t)
        derived_stats = {
    "Total Code Points": len(code_points_array),
    "RGI Emoji Sequences": count_matches(RE_DERIVED["Emoji"], t),
    "Whitespace (Total)": count_matches(RE_DERIVED["Whitespace"], t)
    }

        minor_stats = {}

        if is_honest_mode:
        # --- HONEST MODE (Fixes 1 & 2) ---
        # 1. Count 29 categories against the FULL text (no emoji deletion)
            for key, regex in RE_MINOR_29_HONEST.items():
                minor_stats[key] = count_matches(regex, t)
        # 2. Calculate 'Cn' as the mathematical remainder
            sum_of_29_cats = sum(minor_stats.values())
            total_code_points = derived_stats["Total Code Points"]
            cn_count = total_code_points - sum_of_29_cats
            minor_stats["Cn"] = cn_count
        else:
        # --- LEGACY (FILTERED) MODE ---
        # 1. Keep the original flawed logic (delete emoji)
            text_no_emoji = window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")
        # 2. Count all 30 categories (incl. flawed \p{Cn}) against the filtered text
            for key, regex in RE_MINOR_LEGACY.items():
                minor_stats[key] = count_matches(regex, text_no_emoji)
   # --- PASS 3: Aggregate MAJOR Categories (Tier 2) ---
        # This logic is UNCHANGED. It correctly aggregates 'minor_stats'
        # from whichever mode was run.
        major_stats = {
        "L (Letter)": minor_stats["Lu"] + minor_stats["Ll"] + minor_stats["Lt"] + minor_stats["Lm"] + minor_stats["Lo"],
        "M (Mark)": minor_stats["Mn"] + minor_stats["Mc"] + minor_stats["Me"],
        "N (Number)": minor_stats["Nd"] + minor_stats["Nl"] + minor_stats["No"],
        "P (Punctuation)": minor_stats["Pc"] + minor_stats["Pd"] + minor_stats["Ps"] + minor_stats["Pe"] + minor_stats["Pi"] + minor_stats["Pf"] + minor_stats["Po"],
        "S (Symbol)": minor_stats["Sm"] + minor_stats["Sc"] + minor_stats["Sk"] + minor_stats["So"],
        "Z (Separator)": minor_stats["Zs"] + minor_stats["Zl"] + minor_stats["Zp"],
        "C (Other)": minor_stats["Cc"] + minor_stats["Cf"] + minor_stats["Cs"] + minor_stats["Co"] + minor_stats["Cn"]
        }

        # --- Build Summary (Tier 1) ---
        # This logic is UNCHANGED.
        summary_stats = {
        "Total Code Points": derived_stats["Total Code Points"],
        "RGI Emoji Sequences": derived_stats["RGI Emoji Sequences"],
        "Whitespace (Total)": derived_stats["Whitespace (Total)"],
        "L (Letter)": major_stats["L (Letter)"],
        "N (Number)": major_stats["N (Number)"],
        "P (Punctuation)": major_stats["P (Punctuation)"],
        "S (Symbol)": major_stats["S (Symbol)"]
        }

        return summary_stats, major_stats, minor_stats

def compute_sequence_stats(t: str, is_minor: bool):
    """Module 2: Runs the Token Shape Analysis."""
    if is_minor:
        counters = {key: 0 for key in MINOR_CATEGORIES}
    else:
        counters = {key: 0 for key in MAJOR_CATEGORIES_TEST}
    
    if not t:
        return counters

    current_state = "NONE"
    
    for char in t:
        new_state = get_char_type(char, is_minor)
        
        if new_state != current_state:
            if current_state in counters:
                counters[current_state] += 1
            current_state = new_state
            
    if current_state in counters:
        counters[current_state] += 1
        
    return counters

def compute_script_stats(t: str, is_honest_mode: bool):
    """Module 3: Runs the Script Analysis."""
    
    text_to_scan = t # Default to Honest mode

    if not is_honest_mode:
    # Legacy (Filtered) mode: remove emoji first
        text_to_scan = window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")
    
    script_stats = {}
    sum_of_known_scripts = 0

    # 1. Count the known scripts (Latin, Common, Inherited)
    for key, regex in RE_SCRIPTS.items():
        count = count_matches(regex, text_to_scan)
        script_stats[key] = count
        sum_of_known_scripts += count

    # 2. Get the total code points *of the text we just scanned*
    # This is the key to the "Honest" partition
    total_code_points_in_scan = len(window.Array.from_(text_to_scan))

    # 3. Calculate "Other" as the mathematical remainder
    other_count = total_code_points_in_scan - sum_of_known_scripts
    script_stats["Other"] = other_count
    
    return script_stats

def compute_grapheme_stats(t: str):
    """Module 1 (Alternate): Runs the 3-Tier analysis AND
       Module 1.5: Grapheme Forensics."""
    
    # --- PASS 1: Get all grapheme segments ---
    segments_iterable = GRAPHEME_SEGMENTER.segment(t)
    segments = window.Array.from_(segments_iterable)
    
    total_graphemes = len(segments)
    
    # Initialize counters for Module 1
    minor_stats = {key: 0 for key in MINOR_CATEGORIES}

    # Initialize counters for Module 1.5 (Forensics)
    single_cp_count = 0
    multi_cp_count = 0
    total_mark_count = 0
    max_marks = 0

    # --- PASS 2: Classify and Analyze each grapheme ---
    for segment in segments:
        grapheme_str = segment.segment
        
        if not grapheme_str:
            continue

        # --- Module 1.5 Logic (Forensics) ---
        cp_array = window.Array.from_(grapheme_str)
        cp_count = len(cp_array)

        if cp_count == 1:
            single_cp_count += 1
        elif cp_count > 1:
            multi_cp_count += 1
        
        mark_count = count_matches(RE_MARK_COUNTER, grapheme_str)
        total_mark_count += mark_count
        if mark_count > max_marks:
            max_marks = mark_count

        # --- Module 1 Logic (Classification) ---
        first_char = cp_array[0]
        
        classified = False
        for key, regex in TEST_MINOR.items():
            if regex.test(first_char):
                minor_stats[key] += 1
                classified = True
                break
        
        if not classified:
            minor_stats["Cn"] += 1

    # --- PASS 3: Aggregate MAJOR Categories (Tier 2) ---
    major_stats = {
        "L (Letter)": minor_stats["Lu"] + minor_stats["Ll"] + minor_stats["Lt"] + minor_stats["Lm"] + minor_stats["Lo"],
        "M (Mark)": minor_stats["Mn"] + minor_stats["Mc"] + minor_stats["Me"],
        "N (Number)": minor_stats["Nd"] + minor_stats["Nl"] + minor_stats["No"],
        "P (Punctuation)": minor_stats["Pc"] + minor_stats["Pd"] + minor_stats["Ps"] + minor_stats["Pe"] + minor_stats["Pi"] + minor_stats["Pf"] + minor_stats["Po"],
        "S (Symbol)": minor_stats["Sm"] + minor_stats["Sc"] + minor_stats["Sk"] + minor_stats["So"],
        "Z (Separator)": minor_stats["Zs"] + minor_stats["Zl"] + minor_stats["Zp"],
        "C (Other)": minor_stats["Cc"] + minor_stats["Cf"] + minor_stats["Cs"] + minor_stats["Co"] + minor_stats["Cn"]
    }

    # --- Build Summary (Tier 1) ---
    summary_stats = {
        "Total Graphemes": total_graphemes,
        "L (Letter)": major_stats["L (Letter)"],
        "N (Number)": major_stats["N (Number)"],
        "P (Punctuation)": major_stats["P (Punctuation)"],
        "S (Symbol)": major_stats["S (Symbol)"]
    }

    # --- PASS 4: Build Grapheme Forensics (Module 1.5) ---
    avg_marks = (total_mark_count / total_graphemes) if total_graphemes > 0 else 0
    
    grapheme_forensic_stats = {
        "Total Graphemes": total_graphemes,
        "Single-Code-Point": single_cp_count,
        "Multi-Code-Point": multi_cp_count,
        "Total Combining Marks": total_mark_count,
        "Max Marks in one Grapheme": max_marks, # (Zalgo Detector)
        "Avg. Marks per Grapheme": round(avg_marks, 2)
    }

    # Return all 4 data packages
    return summary_stats, major_stats, minor_stats, grapheme_forensic_stats
    
def _get_confusable_skeleton(char: str) -> str | None:
    """
    Checks if a character is confusable and returns its skeleton.
    Returns None if it's not confusable.
    """
    
    # 1. Check the main map first (fastest)
    skel = CONFUSABLES_MAP.get(ord(char))
    if skel:
        return skel
    
    # 2. Check NFKC normalization (slower, but catches symbols/width)
    try:
        # Only normalize categories that make sense
        if unicodedata.category(char)[0] in ("L", "N", "S", "P"):
            normalized_char = unicodedata.normalize('NFKC', char)
            if normalized_char != char:
                # Return the normalized version as the skeleton
                return normalized_char
    except Exception:
        pass # Not a valid char for normalization
        
    return None

def compute_confusable_stats(t: str, is_honest_mode: bool):
    """
    Module 7 — Confusables & Spoofing: use the SAME logic for counting and
    for the visual report (map + NFKC). This keeps the card and the report in sync.
    """
    if not t:
        return {}, []

    # PASS 1 — per-character count (respects legacy emoji filtering)
    text_to_scan = t if is_honest_mode else window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")
    confusable_count = sum(1 for ch in text_to_scan if _get_confusable_skeleton(ch) is not None)
    stats = {"Confusable Chars": confusable_count}

    # PASS 2 — build runs/skeletons for display (now same logic as pass 1)
    runs_detail = []
    alnum_runs_js = window.String.prototype.match.call(t, RE_LN_RUNS)  # includes L|N|S|P
    if not alnum_runs_js:
        return stats, []

    alnum_runs = window.Array.from_(alnum_runs_js)
    idx = 0
    for run in alnum_runs:
        run_py = str(run)

        start = t.find(run_py, idx)
        if start == -1:
            start = t.find(run_py, 0)
        idx = start + len(run_py)

        has_confusable = False
        skel_buf = []
        for ch in run_py:
            sk = _get_confusable_skeleton(ch)
            if sk is not None:
                has_confusable = True
                skel_buf.append(sk)
            else:
                skel_buf.append(ch)

        if has_confusable:
            runs_detail.append({
                "orig": run_py,
                "skel": "".join(skel_buf),
                "start": start,
                "end": start + len(run_py)
            })

    return stats, runs_detail


    
# --- NEW: Standardized Variant Analysis (Module 8) ---
def compute_variant_stats(t: str, is_honest_mode: bool):
    """Module 8: Counts variant base chars and selectors."""
    
    if (not VARIANT_BASE_CHARS and not VARIATION_SELECTORS) or not t:
        return {}

    text_to_scan = t
    if not is_honest_mode:
        # Respect the legacy filter mode
        text_to_scan = window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")

    base_count = 0
    selector_count = 0
    
    # --- DEBUGGING: Print the size of the sets ---
    # print(f"Compute: Base set size: {len(VARIANT_BASE_CHARS)}")
    # print(f"Compute: Selector set size: {len(VARIATION_SELECTORS)}")
    
    for char in text_to_scan:
        cp = ord(char)
        if cp in VARIANT_BASE_CHARS:
            base_count += 1
            # print(f"Found base char: {char}")
            
        if cp in VARIATION_SELECTORS:
            selector_count += 1
            # print(f"Found selector: U+{cp:04X}")
            
    stats = {
        "Variant Base Chars": base_count,
        "Variation Selectors": selector_count
    }
    return stats
    
# NEW: FORENSIC ANALYSIS FUNCTION
def compute_forensic_stats(t: str, is_honest_mode: bool, minor_stats: dict):
    """Module 3 Add-on: Runs the Forensic Analysis."""
    
    text_to_scan = t # Default to Honest mode

    if not is_honest_mode:
        # Legacy (Filtered) mode: remove emoji first
        text_to_scan = window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")
    
    # 1. Initialize with the pre-calculated counts from Module 1
    #    This is efficient and respects "Honest Mode" logic for Cn.
    forensic_stats = {
        # Get counts from minor_stats, defaulting to 0
        "Unassigned (Void)": minor_stats.get("Cn", 0),
        "Surrogates (Broken)": minor_stats.get("Cs", 0),
        "Private Use (Black Box)": minor_stats.get("Co", 0)
    }

    # 2. Run the other regex-based forensic checks
    for key, regex in RE_FORENSIC.items():
        forensic_stats[key] = count_matches(regex, text_to_scan)

    return forensic_stats

# NEW: UAX #44 ANALYSIS FUNCTION
def compute_uax44_stats(t: str, is_honest_mode: bool):
    """Module 5: Runs the UAX #44 Property Analysis."""
    
    text_to_scan = t # Default to Honest mode

    if not is_honest_mode:
        # Legacy (Filtered) mode: remove emoji first
        text_to_scan = window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")
    
    uax44_stats = {}

    for key, regex in RE_UAX44.items():
        uax44_stats[key] = count_matches(regex, text_to_scan)

    return uax44_stats

# NEW: UCD DEEP SCAN FUNCTION (PYTHON)
def compute_ucd_deep_scan(t: str, is_honest_mode: bool):
    """Module 6: Runs a Python-based deep scan for properties
       not available in RegExp."""
    
    text_to_scan = t # Default to Honest mode

    if not is_honest_mode:
        text_to_scan = window.String.prototype.replace.call(t, RE_DERIVED["Emoji"], "")
    
    numeric_type_stats = {}
    numeric_total_value = 0
    number_script_zeros = set()
    id_type_stats = {} # <-- NEW: Phase 2 refactored counter

    # --- NEW: Counters for Phase 1 ---
    block_stats = {}
    age_stats = {}
    
    for char in text_to_scan:
        cp = ord(char) # Get the code point
        
        # --- Phase 1 Logic (Blocks & Age) ---
        block_name = find_in_ranges(cp, BLOCK_STARTS, BLOCK_ENDS, BLOCK_RANGES_STORE)
        if block_name:
            key = f"Block: {block_name}"
            block_stats[key] = block_stats.get(key, 0) + 1
        
        age = find_in_ranges(cp, AGE_STARTS, AGE_ENDS, AGE_RANGES_STORE)
        if age:
            key = f"Age: {age}"
            age_stats[key] = age_stats.get(key, 0) + 1

        # --- NEW: Phase 2 Refactored Logic ---
        # We now look up the Type for *every* char, not just restricted ones
        if char != ' ':
            type_val = find_in_ranges(cp, ID_TYPE_STARTS, ID_TYPE_ENDS, ID_TYPE_RANGES_STORE)
            
            if type_val:
                # We will *only* report on types that are forensically
                # interesting, to avoid noise like "Recommended".
                # "Format" (like Bidi) and "Technical" (like Cc) will now be caught.
                if type_val not in ("Recommended", "Inclusion"):
                    key = f"Type: {type_val}"
                    id_type_stats[key] = id_type_stats.get(key, 0) + 1

        # --- Get Numeric Properties (existing) ---
        try:
            value = unicodedata.numeric(char)  
            numeric_total_value += value
            
            gc = unicodedata.category(char)
            if gc == "Nd":
                n_type = "Num Type: Decimal (Nd)"
                zero_code_point = ord(char) - int(value)
                number_script_zeros.add(zero_code_point)
            elif gc == "Nl": n_type = "Num Type: Letter (Nl)"
            elif gc == "No": n_type = "Num Type: Other (No)"
            else: n_type = f"Num Type: Other ({gc})"
            
            numeric_type_stats[n_type] = numeric_type_stats.get(n_type, 0) + 1
            
        except (ValueError, TypeError):
            pass

    final_total_value = round(numeric_total_value, 4)
        
    final_stats = {
        "Mixed-Number Systems": 1 if len(number_script_zeros) > 1 else 0
        }
        
    return final_stats, numeric_type_stats, final_total_value, block_stats, age_stats, id_type_stats

# --- 5. DOM/UI FUNCTIONS ---

def render_stats(stats_dict, element_id, class_name="card"):
    """Generates and injects HTML for standard stat cards."""
    html = []
    for k, v in stats_dict.items():
        if v > 0:
            html.append(f'<div class="{class_name}"><strong>{k}</strong><div>{v}</div></div>')
    
    element = document.getElementById(element_id)
    if element:
        element.innerHTML = "".join(html) if html else "<p style='padding: 1rem; color: #6b7280;'>No data for this category.</p>"

def render_minor_stats(stats_dict, element_id):
    """Generates and injects HTML for the 30-category breakdown."""
    html = []
    for k, v in stats_dict.items():
        if v > 0:
            alias = ALIASES.get(k, "Unknown")
            html.append(f'<div class="minor-card"><div class="minor-key">{k}</div><div class="minor-alias" title="{alias}">{alias}</div><div class="minor-val">{v}</div></div>')
    
    element = document.getElementById(element_id)
    if element:
        element.innerHTML = "".join(html) if html else "<p style='padding: 1rem; color: #6b7280;'>No data for this category.</p>"

def _select_tab_internal(tab_name):
    """Internal logic for switching tabs."""
    # Store state
    set_state("active_tab", tab_name)
    
    # Update button active class
    for btn in document.querySelectorAll(".tab-btn"):
        btn.classList.remove("active")
    document.getElementById(f"tab-btn-{tab_name}").classList.add("active")
    
    # Update content active class
    for content in document.querySelectorAll(".tab-content"):
        content.classList.remove("active")
    document.getElementById(f"tab-{tab_name}").classList.add("active")

# This is the new event handler for py-click
@create_proxy
def select_tab(event):
    """Handles tab switching UI from a JS click event."""
    # Get tab name from the button's ID
    # e.g., id="tab-btn-summary" -> "summary"
    tab_name = event.target.id.split('-')[-1]
    _select_tab_internal(tab_name)


# --- 6. MAIN ENTRY POINT ---

def update_all(event=None):
    """The main function called on every keypress."""
    t = document.getElementById("text").value

    # Read UI state
    is_minor_seq = document.getElementById("granularity-toggle").checked
    is_honest_mode = document.getElementById("analysis-mode-toggle").checked
    is_grapheme_mode = document.getElementById("unit-mode-toggle").checked
    active_tab = get_state()["active_tab"]

    # Get references to all modules
    module_2_header = document.getElementById("module-2-header")
    module_2_content = document.getElementById("sequence-stats")
    module_2_toggle = document.getElementById("granularity-toggle").parentElement
    
    module_3_header = document.getElementById("module-3-header")
    module_3_content = document.getElementById("script-stats")
    
    module_4_header = document.getElementById("module-4-header")
    module_4_content = document.getElementById("forensic-stats")
    
    module_5_header = document.getElementById("module-5-header")
    module_5_content = document.getElementById("uax44-stats")
    
    module_6_header = document.getElementById("module-6-header")
    module_6_content = document.getElementById("ucd-deep-scan-stats")
    
    grapheme_forensics_module = document.getElementById("grapheme-forensics-module")

    # --- NEW: Module 7 (Phase 3) ---
    module_7_header = document.getElementById("module-7-header")
    module_7_content = document.getElementById("confusable-stats")
    skeleton_output = document.getElementById("skeleton-output")

    # --- NEW: Module 8 (Phase 4) ---
    module_8_header = document.getElementById("module-8-header")
    module_8_content = document.getElementById("intentional-stats-output")
    
    # --- Module 1 Toggle ---
    if is_grapheme_mode:
        # 1. Run compute function (now returns 4 items)
        summary_stats, major_stats, minor_stats, g_forensics = compute_grapheme_stats(t)
        
        # 2. Render Module 1 (the 3 tabs)
        render_stats(summary_stats, "tab-summary")
        render_stats(major_stats, "tab-major")
        render_minor_stats(minor_stats, "tab-minor")
        
        # 3. Show Module 1.5 (Grapheme Forensics)
        grapheme_forensics_module.style.display = "block"
        render_stats(g_forensics, "grapheme-forensics-stats", class_name="card")

        # --- HIDE OTHER MODULES ---
        document.getElementById("analysis-mode-toggle").disabled = True
        module_2_header.style.display = "none"
        module_2_content.style.display = "none"
        module_2_toggle.parentElement.style.display = "none"
        module_3_header.style.display = "none"
        module_3_content.style.display = "none"
        module_4_header.style.display = "none"
        module_4_content.style.display = "none"
        module_5_header.style.display = "none"
        module_5_content.style.display = "none"
        module_6_header.style.display = "none"
        module_6_content.style.display = "none"
        module_7_header.style.display = "none"
        module_7_content.style.display = "none"
        skeleton_output.style.display = "none"
        module_8_header.style.display = "none"
        module_8_content.style.display = "none"
        document.getElementById("intentional-skeleton-output").style.display = "none"
    
    else:
        # --- HIDE GRAPHEME FORENSICS ---
        grapheme_forensics_module.style.display = "none"

        # --- SHOW OTHER MODULES ---
        document.getElementById("analysis-mode-toggle").disabled = False
        module_2_header.style.display = "flex"
        module_2_content.style.display = "grid"
        module_2_toggle.parentElement.style.display = "flex"
        module_3_header.style.display = "flex"
        module_3_content.style.display = "grid"
        module_4_header.style.display = "flex"
        module_4_content.style.display = "grid"
        module_5_header.style.display = "flex"
        module_5_content.style.display = "grid"
        module_6_header.style.display = "flex"
        module_6_content.style.display = "grid"
        module_7_header.style.display = "flex"
        module_7_content.style.display = "grid"
        # skeleton_output display is handled by its content
        module_8_header.style.display = "flex"
        module_8_content.style.display = "grid"
        document.getElementById("intentional-skeleton-output").style.display = "none"
        # --- Original Code Path ---
        summary_stats, major_stats, minor_stats = compute_comprehensive_stats(t, is_honest_mode)

    # Render Module 1 (Always runs, just with different data)
    render_stats(summary_stats, "tab-summary")
    render_stats(major_stats, "tab-major")
    render_minor_stats(minor_stats, "tab-minor")

    # --- Run Modules 2-6 (Only if not in Grapheme mode) ---
    if not is_grapheme_mode:
        t = document.getElementById("text").value # Re-get text
        
        # Run Module 2
        seq_stats = compute_sequence_stats(t, is_minor_seq)
        render_stats(seq_stats, "sequence-stats")

        # Run Module 3 (Script Analysis)
        script_stats = compute_script_stats(t, is_honest_mode)
        # 'compute_security_stats' is now removed (Phase 3)
        combined_script_stats = {
            **script_stats
        }
        render_stats(combined_script_stats, "script-stats", class_name="card")
        
        # Run Module 4 (Forensics)
        forensic_stats = compute_forensic_stats(
            t, is_honest_mode, minor_stats
        )
        render_stats(forensic_stats, "forensic-stats", class_name="card")
        
        # Run Module 5 (UAX #44)
        uax44_stats = compute_uax44_stats(t, is_honest_mode)
        render_stats(uax44_stats, "uax44-stats", class_name="card")

        # Run Module 6 (Python Deep Scan)
        security_flags, numeric_types, total_value, block_stats, age_stats, id_types = compute_ucd_deep_scan(t, is_honest_mode) # <-- Phase 2 Refactor
        
        total_value_dict = {}
        if total_value > 0 or "Num Type: Decimal (Nd)" in numeric_types or "Num Type: Letter (Nl)" in numeric_types or "Num Type: Other (No)" in numeric_types:
            total_value_dict = {"Total Numeric Value": total_value}
            
        combined_deep_scan_stats = {
            **security_flags,       # (Holds "Mixed-Number Systems")
            **id_types,             # <-- NEW: Phase 2 Refactor
            **total_value_dict, 
            **numeric_types,
            **age_stats,            # <-- NEW: Phase 1
            **block_stats           # <-- NEW: Phase 1
        }
    
        render_stats(combined_deep_scan_stats, "ucd-deep-scan-stats", class_name="card")
        
        # --- Run Module 7 (Confusables) ---
        confusable_stats, runs = compute_confusable_stats(t, is_honest_mode)
        render_stats(confusable_stats, "confusable-stats")

        # --- Build the visual report WITHOUT zipping (supports multi-char skeletons) ---
lines = []
for r in runs:
    diff_parts = []
    skel_hex_parts = []

    # Build highlight per original code point
    for ch in r["orig"]:
        skel = _get_confusable_skeleton(ch)  # may be None or multi-char string
        if skel:
            title = 'U+{0:04X} → '.format(ord(ch)) + " ".join('U+{0:04X}'.format(ord(x)) for x in skel)
            diff_parts.append(f'<span class="changed" title="{title}">{skel}</span>')
            skel_hex_parts.extend('U+{0:04X}'.format(ord(x)) for x in skel)
        else:
            diff_parts.append(ch)
            skel_hex_parts.append('U+{0:04X}'.format(ord(ch)))

    # Hex for the originals
    orig_hex = " ".join('U+{0:04X}'.format(ord(c)) for c in r["orig"])
    skel_hex = " ".join(skel_hex_parts)

    lines.append(
        f"{r['orig']} → {''.join(diff_parts)}   "
        f"[{r['start']}–{r['end']}]   "
        f"[Hex: {orig_hex} → {skel_hex}]"
    )

if lines:
    skeleton_output.innerHTML = "\n".join(lines)
    skeleton_output.style.display = "block"
else:
    skeleton_output.style.display = "none"


        # --- Run Module 8 (Standardized Variant Analysis) ---
        variant_stats = compute_variant_stats(t, is_honest_mode)
        render_stats(variant_stats, "intentional-stats-output")

    
    # --- Final UI sync ---
    _select_tab_internal(active_tab)

def _html_escape(s: str) -> str:
    return s.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def _hex_seq(s: str) -> str:
    return " ".join(f"U+{ord(c):04X}" for c in s)

# Initial call to populate the dashboard on load
# update_all()

# NEW: Start loading the external data
asyncio.ensure_future(load_unicode_data())
    
  </script>

</body>

</html>
